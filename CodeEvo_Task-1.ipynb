{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1IVO0zZ8R-tjIAT0kSKdBheRJHrRaBsDf","authorship_tag":"ABX9TyOSC4H7tfyEuM2kFjBPnDbX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gJOh953LxDuo","executionInfo":{"status":"ok","timestamp":1730700741146,"user_tz":-330,"elapsed":17525,"user":{"displayName":"Jagadeeshwar Gillapally","userId":"11552971825427802311"}},"outputId":"e8a69bc5-c889-4fcd-e51d-27f47fd22630"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy: 93.12%\n"]}],"source":["import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","import nltk\n","\n","# Download required NLTK data\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Load the datasets\n","fake_data = pd.read_csv('/content/drive/MyDrive/Dataset/Fake.csv')  # Replace 'fake.csv' with your actual file path if different\n","real_data = pd.read_csv('/content/drive/MyDrive/Dataset/True.csv')  # Replace 'true.csv' with your actual file path if different\n","\n","# Add a 'label' column to each dataset: 1 for fake, 0 for real\n","fake_data['label'] = 1\n","real_data['label'] = 0\n","\n","# Combine the datasets\n","data = pd.concat([fake_data, real_data], ignore_index=True)\n","\n","# Keep only the relevant columns\n","data = data[['text', 'label']]\n","\n","# Define stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Preprocessing function\n","def preprocess_text(text):\n","    text = text.lower()  # Convert to lowercase\n","    text = re.sub(r'\\W', ' ', text)  # Remove non-word characters\n","    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n","    tokens = word_tokenize(text)  # Tokenize\n","    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n","    return ' '.join(tokens)\n","\n","# Apply preprocessing to the text data\n","data['text'] = data['text'].apply(preprocess_text)\n","\n","# TF-IDF Vectorization\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","X = tfidf_vectorizer.fit_transform(data['text']).toarray()\n","y = data['label']\n","\n","# Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Model Training with Naive Bayes\n","model = MultinomialNB()\n","model.fit(X_train, y_train)\n","\n","# Model Evaluation\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy * 100:.2f}%')\n"]}]}